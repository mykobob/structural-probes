{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.5.0 available.\n",
      "INFO:transformers.file_utils:TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../structural-probes/')\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers.comet import CometLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from utils.setup_runs import parse_args, get_default_args, get_comet_key\n",
    "from finetune_bert_module import SST_Test\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_hparams = {\n",
    "    'lr': 7e-3,\n",
    "    'batch_size': 48,\n",
    "}\n",
    "desired_params = {}\n",
    "hparams, args = get_default_args(desired_hparams, desired_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to cuda if available unless explicitly disabled\n",
    "if args.device == torch.device('cuda'):\n",
    "    num_gpus = 1\n",
    "else:\n",
    "    num_gpus = 0\n",
    "\n",
    "# Output debug logs in debug mode\n",
    "if args.debug:\n",
    "    args.log_level = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all seeds manually for consistent results\n",
    "torch.manual_seed(hparams.seed)\n",
    "np.random.seed(hparams.seed)\n",
    "random.seed(hparams.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:CometLogger will be initialized in online mode\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/mykobob/structural-probes-extension/1d8f1b6ff76341259210aa48cb1fff5c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# CometLogger configuration\n",
    "###############\n",
    "\n",
    "comet_key = get_comet_key()\n",
    "comet_logger = CometLogger(\n",
    "    api_key = comet_key,\n",
    "    workspace = \"mykobob\",\n",
    "    project_name = \"structural-probes-extension\",\n",
    "    experiment_name = args.run_name\n",
    ")\n",
    "# Log args to comet\n",
    "comet_logger.log_hyperparams(hparams)\n",
    "comet_logger.experiment.set_name(args.run_name)\n",
    "comet_logger.experiment.add_tag(\"sst-tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Other callback configuration\n",
    "###############\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath= os.path.join(\"lightning_logs\", args.run_name, \"checkpoints\"),\n",
    "    save_top_k=args.num_saved_models,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.early_stopping:\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.00,\n",
    "        patience=args.early_stopping,\n",
    "        verbose=False,\n",
    "        mode='min'\n",
    "        #mode='max'\n",
    "    )\n",
    "else:\n",
    "    early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /home/mli/.cache/torch/transformers/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /home/mli/.cache/torch/transformers/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json from cache at /home/mli/.cache/torch/transformers/90deb4d9dd705272dc4b3db1364d759d551d72a9f70a91f60e3a1f5e278b985d.9019d8d0ae95e32b896211ae7ae130d7c36bb19ccf35c90a9e51923309458f70\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin from cache at /home/mli/.cache/torch/transformers/56c451878be53ca1e310764d1e8312301f3d921378919467947ddd53fef6ba2b.b5f1c2070e0a0c189ca3b08270b0cb5bd0635b7319e74e93bd0dc26689953c27\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Model creation\n",
    "###############\n",
    "\n",
    "testing = False\n",
    "if testing:\n",
    "    model = SST_Test.load_from_metrics(\n",
    "            weights_path='lightning_logs/regression_training/_ckpt_epoch_19',\n",
    "            map_location=None)\n",
    "else:\n",
    "    model = SST_Test(args, hparams).to(args.device)\n",
    "\n",
    "comet_logger.experiment.add_tag(\"SST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Create Trainer with specified attributes\n",
    "###############\n",
    "\n",
    "trainer = Trainer(\n",
    "    fast_dev_run=args.debug,\n",
    "    max_nb_epochs=hparams.epochs,\n",
    "    gpus=num_gpus,\n",
    "    train_percent_check=hparams.train_pct,\n",
    "    val_percent_check=hparams.val_pct,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    early_stop_callback=early_stopping,\n",
    "    logger=comet_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "    | Name                                                  | Type                          | Params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0   | bert                                                  | BertForSequenceClassification | 333 M \n",
      "1   | bert.bert                                             | BertModel                     | 333 M \n",
      "2   | bert.bert.embeddings                                  | BertEmbeddings                | 30 M  \n",
      "3   | bert.bert.embeddings.word_embeddings                  | Embedding                     | 29 M  \n",
      "4   | bert.bert.embeddings.position_embeddings              | Embedding                     | 524 K \n",
      "5   | bert.bert.embeddings.token_type_embeddings            | Embedding                     | 2 K   \n",
      "6   | bert.bert.embeddings.LayerNorm                        | LayerNorm                     | 2 K   \n",
      "7   | bert.bert.embeddings.dropout                          | Dropout                       | 0     \n",
      "8   | bert.bert.encoder                                     | BertEncoder                   | 302 M \n",
      "9   | bert.bert.encoder.layer                               | ModuleList                    | 302 M \n",
      "10  | bert.bert.encoder.layer.0                             | BertLayer                     | 12 M  \n",
      "11  | bert.bert.encoder.layer.0.attention                   | BertAttention                 | 4 M   \n",
      "12  | bert.bert.encoder.layer.0.attention.self              | BertSelfAttention             | 3 M   \n",
      "13  | bert.bert.encoder.layer.0.attention.self.query        | Linear                        | 1 M   \n",
      "14  | bert.bert.encoder.layer.0.attention.self.key          | Linear                        | 1 M   \n",
      "15  | bert.bert.encoder.layer.0.attention.self.value        | Linear                        | 1 M   \n",
      "16  | bert.bert.encoder.layer.0.attention.self.dropout      | Dropout                       | 0     \n",
      "17  | bert.bert.encoder.layer.0.attention.output            | BertSelfOutput                | 1 M   \n",
      "18  | bert.bert.encoder.layer.0.attention.output.dense      | Linear                        | 1 M   \n",
      "19  | bert.bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "20  | bert.bert.encoder.layer.0.attention.output.dropout    | Dropout                       | 0     \n",
      "21  | bert.bert.encoder.layer.0.intermediate                | BertIntermediate              | 4 M   \n",
      "22  | bert.bert.encoder.layer.0.intermediate.dense          | Linear                        | 4 M   \n",
      "23  | bert.bert.encoder.layer.0.output                      | BertOutput                    | 4 M   \n",
      "24  | bert.bert.encoder.layer.0.output.dense                | Linear                        | 4 M   \n",
      "25  | bert.bert.encoder.layer.0.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "26  | bert.bert.encoder.layer.0.output.dropout              | Dropout                       | 0     \n",
      "27  | bert.bert.encoder.layer.1                             | BertLayer                     | 12 M  \n",
      "28  | bert.bert.encoder.layer.1.attention                   | BertAttention                 | 4 M   \n",
      "29  | bert.bert.encoder.layer.1.attention.self              | BertSelfAttention             | 3 M   \n",
      "30  | bert.bert.encoder.layer.1.attention.self.query        | Linear                        | 1 M   \n",
      "31  | bert.bert.encoder.layer.1.attention.self.key          | Linear                        | 1 M   \n",
      "32  | bert.bert.encoder.layer.1.attention.self.value        | Linear                        | 1 M   \n",
      "33  | bert.bert.encoder.layer.1.attention.self.dropout      | Dropout                       | 0     \n",
      "34  | bert.bert.encoder.layer.1.attention.output            | BertSelfOutput                | 1 M   \n",
      "35  | bert.bert.encoder.layer.1.attention.output.dense      | Linear                        | 1 M   \n",
      "36  | bert.bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "37  | bert.bert.encoder.layer.1.attention.output.dropout    | Dropout                       | 0     \n",
      "38  | bert.bert.encoder.layer.1.intermediate                | BertIntermediate              | 4 M   \n",
      "39  | bert.bert.encoder.layer.1.intermediate.dense          | Linear                        | 4 M   \n",
      "40  | bert.bert.encoder.layer.1.output                      | BertOutput                    | 4 M   \n",
      "41  | bert.bert.encoder.layer.1.output.dense                | Linear                        | 4 M   \n",
      "42  | bert.bert.encoder.layer.1.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "43  | bert.bert.encoder.layer.1.output.dropout              | Dropout                       | 0     \n",
      "44  | bert.bert.encoder.layer.2                             | BertLayer                     | 12 M  \n",
      "45  | bert.bert.encoder.layer.2.attention                   | BertAttention                 | 4 M   \n",
      "46  | bert.bert.encoder.layer.2.attention.self              | BertSelfAttention             | 3 M   \n",
      "47  | bert.bert.encoder.layer.2.attention.self.query        | Linear                        | 1 M   \n",
      "48  | bert.bert.encoder.layer.2.attention.self.key          | Linear                        | 1 M   \n",
      "49  | bert.bert.encoder.layer.2.attention.self.value        | Linear                        | 1 M   \n",
      "50  | bert.bert.encoder.layer.2.attention.self.dropout      | Dropout                       | 0     \n",
      "51  | bert.bert.encoder.layer.2.attention.output            | BertSelfOutput                | 1 M   \n",
      "52  | bert.bert.encoder.layer.2.attention.output.dense      | Linear                        | 1 M   \n",
      "53  | bert.bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "54  | bert.bert.encoder.layer.2.attention.output.dropout    | Dropout                       | 0     \n",
      "55  | bert.bert.encoder.layer.2.intermediate                | BertIntermediate              | 4 M   \n",
      "56  | bert.bert.encoder.layer.2.intermediate.dense          | Linear                        | 4 M   \n",
      "57  | bert.bert.encoder.layer.2.output                      | BertOutput                    | 4 M   \n",
      "58  | bert.bert.encoder.layer.2.output.dense                | Linear                        | 4 M   \n",
      "59  | bert.bert.encoder.layer.2.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "60  | bert.bert.encoder.layer.2.output.dropout              | Dropout                       | 0     \n",
      "61  | bert.bert.encoder.layer.3                             | BertLayer                     | 12 M  \n",
      "62  | bert.bert.encoder.layer.3.attention                   | BertAttention                 | 4 M   \n",
      "63  | bert.bert.encoder.layer.3.attention.self              | BertSelfAttention             | 3 M   \n",
      "64  | bert.bert.encoder.layer.3.attention.self.query        | Linear                        | 1 M   \n",
      "65  | bert.bert.encoder.layer.3.attention.self.key          | Linear                        | 1 M   \n",
      "66  | bert.bert.encoder.layer.3.attention.self.value        | Linear                        | 1 M   \n",
      "67  | bert.bert.encoder.layer.3.attention.self.dropout      | Dropout                       | 0     \n",
      "68  | bert.bert.encoder.layer.3.attention.output            | BertSelfOutput                | 1 M   \n",
      "69  | bert.bert.encoder.layer.3.attention.output.dense      | Linear                        | 1 M   \n",
      "70  | bert.bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "71  | bert.bert.encoder.layer.3.attention.output.dropout    | Dropout                       | 0     \n",
      "72  | bert.bert.encoder.layer.3.intermediate                | BertIntermediate              | 4 M   \n",
      "73  | bert.bert.encoder.layer.3.intermediate.dense          | Linear                        | 4 M   \n",
      "74  | bert.bert.encoder.layer.3.output                      | BertOutput                    | 4 M   \n",
      "75  | bert.bert.encoder.layer.3.output.dense                | Linear                        | 4 M   \n",
      "76  | bert.bert.encoder.layer.3.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "77  | bert.bert.encoder.layer.3.output.dropout              | Dropout                       | 0     \n",
      "78  | bert.bert.encoder.layer.4                             | BertLayer                     | 12 M  \n",
      "79  | bert.bert.encoder.layer.4.attention                   | BertAttention                 | 4 M   \n",
      "80  | bert.bert.encoder.layer.4.attention.self              | BertSelfAttention             | 3 M   \n",
      "81  | bert.bert.encoder.layer.4.attention.self.query        | Linear                        | 1 M   \n",
      "82  | bert.bert.encoder.layer.4.attention.self.key          | Linear                        | 1 M   \n",
      "83  | bert.bert.encoder.layer.4.attention.self.value        | Linear                        | 1 M   \n",
      "84  | bert.bert.encoder.layer.4.attention.self.dropout      | Dropout                       | 0     \n",
      "85  | bert.bert.encoder.layer.4.attention.output            | BertSelfOutput                | 1 M   \n",
      "86  | bert.bert.encoder.layer.4.attention.output.dense      | Linear                        | 1 M   \n",
      "87  | bert.bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "88  | bert.bert.encoder.layer.4.attention.output.dropout    | Dropout                       | 0     \n",
      "89  | bert.bert.encoder.layer.4.intermediate                | BertIntermediate              | 4 M   \n",
      "90  | bert.bert.encoder.layer.4.intermediate.dense          | Linear                        | 4 M   \n",
      "91  | bert.bert.encoder.layer.4.output                      | BertOutput                    | 4 M   \n",
      "92  | bert.bert.encoder.layer.4.output.dense                | Linear                        | 4 M   \n",
      "93  | bert.bert.encoder.layer.4.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "94  | bert.bert.encoder.layer.4.output.dropout              | Dropout                       | 0     \n",
      "95  | bert.bert.encoder.layer.5                             | BertLayer                     | 12 M  \n",
      "96  | bert.bert.encoder.layer.5.attention                   | BertAttention                 | 4 M   \n",
      "97  | bert.bert.encoder.layer.5.attention.self              | BertSelfAttention             | 3 M   \n",
      "98  | bert.bert.encoder.layer.5.attention.self.query        | Linear                        | 1 M   \n",
      "99  | bert.bert.encoder.layer.5.attention.self.key          | Linear                        | 1 M   \n",
      "100 | bert.bert.encoder.layer.5.attention.self.value        | Linear                        | 1 M   \n",
      "101 | bert.bert.encoder.layer.5.attention.self.dropout      | Dropout                       | 0     \n",
      "102 | bert.bert.encoder.layer.5.attention.output            | BertSelfOutput                | 1 M   \n",
      "103 | bert.bert.encoder.layer.5.attention.output.dense      | Linear                        | 1 M   \n",
      "104 | bert.bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "105 | bert.bert.encoder.layer.5.attention.output.dropout    | Dropout                       | 0     \n",
      "106 | bert.bert.encoder.layer.5.intermediate                | BertIntermediate              | 4 M   \n",
      "107 | bert.bert.encoder.layer.5.intermediate.dense          | Linear                        | 4 M   \n",
      "108 | bert.bert.encoder.layer.5.output                      | BertOutput                    | 4 M   \n",
      "109 | bert.bert.encoder.layer.5.output.dense                | Linear                        | 4 M   \n",
      "110 | bert.bert.encoder.layer.5.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "111 | bert.bert.encoder.layer.5.output.dropout              | Dropout                       | 0     \n",
      "112 | bert.bert.encoder.layer.6                             | BertLayer                     | 12 M  \n",
      "113 | bert.bert.encoder.layer.6.attention                   | BertAttention                 | 4 M   \n",
      "114 | bert.bert.encoder.layer.6.attention.self              | BertSelfAttention             | 3 M   \n",
      "115 | bert.bert.encoder.layer.6.attention.self.query        | Linear                        | 1 M   \n",
      "116 | bert.bert.encoder.layer.6.attention.self.key          | Linear                        | 1 M   \n",
      "117 | bert.bert.encoder.layer.6.attention.self.value        | Linear                        | 1 M   \n",
      "118 | bert.bert.encoder.layer.6.attention.self.dropout      | Dropout                       | 0     \n",
      "119 | bert.bert.encoder.layer.6.attention.output            | BertSelfOutput                | 1 M   \n",
      "120 | bert.bert.encoder.layer.6.attention.output.dense      | Linear                        | 1 M   \n",
      "121 | bert.bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "122 | bert.bert.encoder.layer.6.attention.output.dropout    | Dropout                       | 0     \n",
      "123 | bert.bert.encoder.layer.6.intermediate                | BertIntermediate              | 4 M   \n",
      "124 | bert.bert.encoder.layer.6.intermediate.dense          | Linear                        | 4 M   \n",
      "125 | bert.bert.encoder.layer.6.output                      | BertOutput                    | 4 M   \n",
      "126 | bert.bert.encoder.layer.6.output.dense                | Linear                        | 4 M   \n",
      "127 | bert.bert.encoder.layer.6.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "128 | bert.bert.encoder.layer.6.output.dropout              | Dropout                       | 0     \n",
      "129 | bert.bert.encoder.layer.7                             | BertLayer                     | 12 M  \n",
      "130 | bert.bert.encoder.layer.7.attention                   | BertAttention                 | 4 M   \n",
      "131 | bert.bert.encoder.layer.7.attention.self              | BertSelfAttention             | 3 M   \n",
      "132 | bert.bert.encoder.layer.7.attention.self.query        | Linear                        | 1 M   \n",
      "133 | bert.bert.encoder.layer.7.attention.self.key          | Linear                        | 1 M   \n",
      "134 | bert.bert.encoder.layer.7.attention.self.value        | Linear                        | 1 M   \n",
      "135 | bert.bert.encoder.layer.7.attention.self.dropout      | Dropout                       | 0     \n",
      "136 | bert.bert.encoder.layer.7.attention.output            | BertSelfOutput                | 1 M   \n",
      "137 | bert.bert.encoder.layer.7.attention.output.dense      | Linear                        | 1 M   \n",
      "138 | bert.bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "139 | bert.bert.encoder.layer.7.attention.output.dropout    | Dropout                       | 0     \n",
      "140 | bert.bert.encoder.layer.7.intermediate                | BertIntermediate              | 4 M   \n",
      "141 | bert.bert.encoder.layer.7.intermediate.dense          | Linear                        | 4 M   \n",
      "142 | bert.bert.encoder.layer.7.output                      | BertOutput                    | 4 M   \n",
      "143 | bert.bert.encoder.layer.7.output.dense                | Linear                        | 4 M   \n",
      "144 | bert.bert.encoder.layer.7.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "145 | bert.bert.encoder.layer.7.output.dropout              | Dropout                       | 0     \n",
      "146 | bert.bert.encoder.layer.8                             | BertLayer                     | 12 M  \n",
      "147 | bert.bert.encoder.layer.8.attention                   | BertAttention                 | 4 M   \n",
      "148 | bert.bert.encoder.layer.8.attention.self              | BertSelfAttention             | 3 M   \n",
      "149 | bert.bert.encoder.layer.8.attention.self.query        | Linear                        | 1 M   \n",
      "150 | bert.bert.encoder.layer.8.attention.self.key          | Linear                        | 1 M   \n",
      "151 | bert.bert.encoder.layer.8.attention.self.value        | Linear                        | 1 M   \n",
      "152 | bert.bert.encoder.layer.8.attention.self.dropout      | Dropout                       | 0     \n",
      "153 | bert.bert.encoder.layer.8.attention.output            | BertSelfOutput                | 1 M   \n",
      "154 | bert.bert.encoder.layer.8.attention.output.dense      | Linear                        | 1 M   \n",
      "155 | bert.bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "156 | bert.bert.encoder.layer.8.attention.output.dropout    | Dropout                       | 0     \n",
      "157 | bert.bert.encoder.layer.8.intermediate                | BertIntermediate              | 4 M   \n",
      "158 | bert.bert.encoder.layer.8.intermediate.dense          | Linear                        | 4 M   \n",
      "159 | bert.bert.encoder.layer.8.output                      | BertOutput                    | 4 M   \n",
      "160 | bert.bert.encoder.layer.8.output.dense                | Linear                        | 4 M   \n",
      "161 | bert.bert.encoder.layer.8.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "162 | bert.bert.encoder.layer.8.output.dropout              | Dropout                       | 0     \n",
      "163 | bert.bert.encoder.layer.9                             | BertLayer                     | 12 M  \n",
      "164 | bert.bert.encoder.layer.9.attention                   | BertAttention                 | 4 M   \n",
      "165 | bert.bert.encoder.layer.9.attention.self              | BertSelfAttention             | 3 M   \n",
      "166 | bert.bert.encoder.layer.9.attention.self.query        | Linear                        | 1 M   \n",
      "167 | bert.bert.encoder.layer.9.attention.self.key          | Linear                        | 1 M   \n",
      "168 | bert.bert.encoder.layer.9.attention.self.value        | Linear                        | 1 M   \n",
      "169 | bert.bert.encoder.layer.9.attention.self.dropout      | Dropout                       | 0     \n",
      "170 | bert.bert.encoder.layer.9.attention.output            | BertSelfOutput                | 1 M   \n",
      "171 | bert.bert.encoder.layer.9.attention.output.dense      | Linear                        | 1 M   \n",
      "172 | bert.bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm                     | 2 K   \n",
      "173 | bert.bert.encoder.layer.9.attention.output.dropout    | Dropout                       | 0     \n",
      "174 | bert.bert.encoder.layer.9.intermediate                | BertIntermediate              | 4 M   \n",
      "175 | bert.bert.encoder.layer.9.intermediate.dense          | Linear                        | 4 M   \n",
      "176 | bert.bert.encoder.layer.9.output                      | BertOutput                    | 4 M   \n",
      "177 | bert.bert.encoder.layer.9.output.dense                | Linear                        | 4 M   \n",
      "178 | bert.bert.encoder.layer.9.output.LayerNorm            | LayerNorm                     | 2 K   \n",
      "179 | bert.bert.encoder.layer.9.output.dropout              | Dropout                       | 0     \n",
      "180 | bert.bert.encoder.layer.10                            | BertLayer                     | 12 M  \n",
      "181 | bert.bert.encoder.layer.10.attention                  | BertAttention                 | 4 M   \n",
      "182 | bert.bert.encoder.layer.10.attention.self             | BertSelfAttention             | 3 M   \n",
      "183 | bert.bert.encoder.layer.10.attention.self.query       | Linear                        | 1 M   \n",
      "184 | bert.bert.encoder.layer.10.attention.self.key         | Linear                        | 1 M   \n",
      "185 | bert.bert.encoder.layer.10.attention.self.value       | Linear                        | 1 M   \n",
      "186 | bert.bert.encoder.layer.10.attention.self.dropout     | Dropout                       | 0     \n",
      "187 | bert.bert.encoder.layer.10.attention.output           | BertSelfOutput                | 1 M   \n",
      "188 | bert.bert.encoder.layer.10.attention.output.dense     | Linear                        | 1 M   \n",
      "189 | bert.bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "190 | bert.bert.encoder.layer.10.attention.output.dropout   | Dropout                       | 0     \n",
      "191 | bert.bert.encoder.layer.10.intermediate               | BertIntermediate              | 4 M   \n",
      "192 | bert.bert.encoder.layer.10.intermediate.dense         | Linear                        | 4 M   \n",
      "193 | bert.bert.encoder.layer.10.output                     | BertOutput                    | 4 M   \n",
      "194 | bert.bert.encoder.layer.10.output.dense               | Linear                        | 4 M   \n",
      "195 | bert.bert.encoder.layer.10.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "196 | bert.bert.encoder.layer.10.output.dropout             | Dropout                       | 0     \n",
      "197 | bert.bert.encoder.layer.11                            | BertLayer                     | 12 M  \n",
      "198 | bert.bert.encoder.layer.11.attention                  | BertAttention                 | 4 M   \n",
      "199 | bert.bert.encoder.layer.11.attention.self             | BertSelfAttention             | 3 M   \n",
      "200 | bert.bert.encoder.layer.11.attention.self.query       | Linear                        | 1 M   \n",
      "201 | bert.bert.encoder.layer.11.attention.self.key         | Linear                        | 1 M   \n",
      "202 | bert.bert.encoder.layer.11.attention.self.value       | Linear                        | 1 M   \n",
      "203 | bert.bert.encoder.layer.11.attention.self.dropout     | Dropout                       | 0     \n",
      "204 | bert.bert.encoder.layer.11.attention.output           | BertSelfOutput                | 1 M   \n",
      "205 | bert.bert.encoder.layer.11.attention.output.dense     | Linear                        | 1 M   \n",
      "206 | bert.bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "207 | bert.bert.encoder.layer.11.attention.output.dropout   | Dropout                       | 0     \n",
      "208 | bert.bert.encoder.layer.11.intermediate               | BertIntermediate              | 4 M   \n",
      "209 | bert.bert.encoder.layer.11.intermediate.dense         | Linear                        | 4 M   \n",
      "210 | bert.bert.encoder.layer.11.output                     | BertOutput                    | 4 M   \n",
      "211 | bert.bert.encoder.layer.11.output.dense               | Linear                        | 4 M   \n",
      "212 | bert.bert.encoder.layer.11.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "213 | bert.bert.encoder.layer.11.output.dropout             | Dropout                       | 0     \n",
      "214 | bert.bert.encoder.layer.12                            | BertLayer                     | 12 M  \n",
      "215 | bert.bert.encoder.layer.12.attention                  | BertAttention                 | 4 M   \n",
      "216 | bert.bert.encoder.layer.12.attention.self             | BertSelfAttention             | 3 M   \n",
      "217 | bert.bert.encoder.layer.12.attention.self.query       | Linear                        | 1 M   \n",
      "218 | bert.bert.encoder.layer.12.attention.self.key         | Linear                        | 1 M   \n",
      "219 | bert.bert.encoder.layer.12.attention.self.value       | Linear                        | 1 M   \n",
      "220 | bert.bert.encoder.layer.12.attention.self.dropout     | Dropout                       | 0     \n",
      "221 | bert.bert.encoder.layer.12.attention.output           | BertSelfOutput                | 1 M   \n",
      "222 | bert.bert.encoder.layer.12.attention.output.dense     | Linear                        | 1 M   \n",
      "223 | bert.bert.encoder.layer.12.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "224 | bert.bert.encoder.layer.12.attention.output.dropout   | Dropout                       | 0     \n",
      "225 | bert.bert.encoder.layer.12.intermediate               | BertIntermediate              | 4 M   \n",
      "226 | bert.bert.encoder.layer.12.intermediate.dense         | Linear                        | 4 M   \n",
      "227 | bert.bert.encoder.layer.12.output                     | BertOutput                    | 4 M   \n",
      "228 | bert.bert.encoder.layer.12.output.dense               | Linear                        | 4 M   \n",
      "229 | bert.bert.encoder.layer.12.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "230 | bert.bert.encoder.layer.12.output.dropout             | Dropout                       | 0     \n",
      "231 | bert.bert.encoder.layer.13                            | BertLayer                     | 12 M  \n",
      "232 | bert.bert.encoder.layer.13.attention                  | BertAttention                 | 4 M   \n",
      "233 | bert.bert.encoder.layer.13.attention.self             | BertSelfAttention             | 3 M   \n",
      "234 | bert.bert.encoder.layer.13.attention.self.query       | Linear                        | 1 M   \n",
      "235 | bert.bert.encoder.layer.13.attention.self.key         | Linear                        | 1 M   \n",
      "236 | bert.bert.encoder.layer.13.attention.self.value       | Linear                        | 1 M   \n",
      "237 | bert.bert.encoder.layer.13.attention.self.dropout     | Dropout                       | 0     \n",
      "238 | bert.bert.encoder.layer.13.attention.output           | BertSelfOutput                | 1 M   \n",
      "239 | bert.bert.encoder.layer.13.attention.output.dense     | Linear                        | 1 M   \n",
      "240 | bert.bert.encoder.layer.13.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "241 | bert.bert.encoder.layer.13.attention.output.dropout   | Dropout                       | 0     \n",
      "242 | bert.bert.encoder.layer.13.intermediate               | BertIntermediate              | 4 M   \n",
      "243 | bert.bert.encoder.layer.13.intermediate.dense         | Linear                        | 4 M   \n",
      "244 | bert.bert.encoder.layer.13.output                     | BertOutput                    | 4 M   \n",
      "245 | bert.bert.encoder.layer.13.output.dense               | Linear                        | 4 M   \n",
      "246 | bert.bert.encoder.layer.13.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "247 | bert.bert.encoder.layer.13.output.dropout             | Dropout                       | 0     \n",
      "248 | bert.bert.encoder.layer.14                            | BertLayer                     | 12 M  \n",
      "249 | bert.bert.encoder.layer.14.attention                  | BertAttention                 | 4 M   \n",
      "250 | bert.bert.encoder.layer.14.attention.self             | BertSelfAttention             | 3 M   \n",
      "251 | bert.bert.encoder.layer.14.attention.self.query       | Linear                        | 1 M   \n",
      "252 | bert.bert.encoder.layer.14.attention.self.key         | Linear                        | 1 M   \n",
      "253 | bert.bert.encoder.layer.14.attention.self.value       | Linear                        | 1 M   \n",
      "254 | bert.bert.encoder.layer.14.attention.self.dropout     | Dropout                       | 0     \n",
      "255 | bert.bert.encoder.layer.14.attention.output           | BertSelfOutput                | 1 M   \n",
      "256 | bert.bert.encoder.layer.14.attention.output.dense     | Linear                        | 1 M   \n",
      "257 | bert.bert.encoder.layer.14.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "258 | bert.bert.encoder.layer.14.attention.output.dropout   | Dropout                       | 0     \n",
      "259 | bert.bert.encoder.layer.14.intermediate               | BertIntermediate              | 4 M   \n",
      "260 | bert.bert.encoder.layer.14.intermediate.dense         | Linear                        | 4 M   \n",
      "261 | bert.bert.encoder.layer.14.output                     | BertOutput                    | 4 M   \n",
      "262 | bert.bert.encoder.layer.14.output.dense               | Linear                        | 4 M   \n",
      "263 | bert.bert.encoder.layer.14.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "264 | bert.bert.encoder.layer.14.output.dropout             | Dropout                       | 0     \n",
      "265 | bert.bert.encoder.layer.15                            | BertLayer                     | 12 M  \n",
      "266 | bert.bert.encoder.layer.15.attention                  | BertAttention                 | 4 M   \n",
      "267 | bert.bert.encoder.layer.15.attention.self             | BertSelfAttention             | 3 M   \n",
      "268 | bert.bert.encoder.layer.15.attention.self.query       | Linear                        | 1 M   \n",
      "269 | bert.bert.encoder.layer.15.attention.self.key         | Linear                        | 1 M   \n",
      "270 | bert.bert.encoder.layer.15.attention.self.value       | Linear                        | 1 M   \n",
      "271 | bert.bert.encoder.layer.15.attention.self.dropout     | Dropout                       | 0     \n",
      "272 | bert.bert.encoder.layer.15.attention.output           | BertSelfOutput                | 1 M   \n",
      "273 | bert.bert.encoder.layer.15.attention.output.dense     | Linear                        | 1 M   \n",
      "274 | bert.bert.encoder.layer.15.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "275 | bert.bert.encoder.layer.15.attention.output.dropout   | Dropout                       | 0     \n",
      "276 | bert.bert.encoder.layer.15.intermediate               | BertIntermediate              | 4 M   \n",
      "277 | bert.bert.encoder.layer.15.intermediate.dense         | Linear                        | 4 M   \n",
      "278 | bert.bert.encoder.layer.15.output                     | BertOutput                    | 4 M   \n",
      "279 | bert.bert.encoder.layer.15.output.dense               | Linear                        | 4 M   \n",
      "280 | bert.bert.encoder.layer.15.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "281 | bert.bert.encoder.layer.15.output.dropout             | Dropout                       | 0     \n",
      "282 | bert.bert.encoder.layer.16                            | BertLayer                     | 12 M  \n",
      "283 | bert.bert.encoder.layer.16.attention                  | BertAttention                 | 4 M   \n",
      "284 | bert.bert.encoder.layer.16.attention.self             | BertSelfAttention             | 3 M   \n",
      "285 | bert.bert.encoder.layer.16.attention.self.query       | Linear                        | 1 M   \n",
      "286 | bert.bert.encoder.layer.16.attention.self.key         | Linear                        | 1 M   \n",
      "287 | bert.bert.encoder.layer.16.attention.self.value       | Linear                        | 1 M   \n",
      "288 | bert.bert.encoder.layer.16.attention.self.dropout     | Dropout                       | 0     \n",
      "289 | bert.bert.encoder.layer.16.attention.output           | BertSelfOutput                | 1 M   \n",
      "290 | bert.bert.encoder.layer.16.attention.output.dense     | Linear                        | 1 M   \n",
      "291 | bert.bert.encoder.layer.16.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "292 | bert.bert.encoder.layer.16.attention.output.dropout   | Dropout                       | 0     \n",
      "293 | bert.bert.encoder.layer.16.intermediate               | BertIntermediate              | 4 M   \n",
      "294 | bert.bert.encoder.layer.16.intermediate.dense         | Linear                        | 4 M   \n",
      "295 | bert.bert.encoder.layer.16.output                     | BertOutput                    | 4 M   \n",
      "296 | bert.bert.encoder.layer.16.output.dense               | Linear                        | 4 M   \n",
      "297 | bert.bert.encoder.layer.16.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "298 | bert.bert.encoder.layer.16.output.dropout             | Dropout                       | 0     \n",
      "299 | bert.bert.encoder.layer.17                            | BertLayer                     | 12 M  \n",
      "300 | bert.bert.encoder.layer.17.attention                  | BertAttention                 | 4 M   \n",
      "301 | bert.bert.encoder.layer.17.attention.self             | BertSelfAttention             | 3 M   \n",
      "302 | bert.bert.encoder.layer.17.attention.self.query       | Linear                        | 1 M   \n",
      "303 | bert.bert.encoder.layer.17.attention.self.key         | Linear                        | 1 M   \n",
      "304 | bert.bert.encoder.layer.17.attention.self.value       | Linear                        | 1 M   \n",
      "305 | bert.bert.encoder.layer.17.attention.self.dropout     | Dropout                       | 0     \n",
      "306 | bert.bert.encoder.layer.17.attention.output           | BertSelfOutput                | 1 M   \n",
      "307 | bert.bert.encoder.layer.17.attention.output.dense     | Linear                        | 1 M   \n",
      "308 | bert.bert.encoder.layer.17.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "309 | bert.bert.encoder.layer.17.attention.output.dropout   | Dropout                       | 0     \n",
      "310 | bert.bert.encoder.layer.17.intermediate               | BertIntermediate              | 4 M   \n",
      "311 | bert.bert.encoder.layer.17.intermediate.dense         | Linear                        | 4 M   \n",
      "312 | bert.bert.encoder.layer.17.output                     | BertOutput                    | 4 M   \n",
      "313 | bert.bert.encoder.layer.17.output.dense               | Linear                        | 4 M   \n",
      "314 | bert.bert.encoder.layer.17.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "315 | bert.bert.encoder.layer.17.output.dropout             | Dropout                       | 0     \n",
      "316 | bert.bert.encoder.layer.18                            | BertLayer                     | 12 M  \n",
      "317 | bert.bert.encoder.layer.18.attention                  | BertAttention                 | 4 M   \n",
      "318 | bert.bert.encoder.layer.18.attention.self             | BertSelfAttention             | 3 M   \n",
      "319 | bert.bert.encoder.layer.18.attention.self.query       | Linear                        | 1 M   \n",
      "320 | bert.bert.encoder.layer.18.attention.self.key         | Linear                        | 1 M   \n",
      "321 | bert.bert.encoder.layer.18.attention.self.value       | Linear                        | 1 M   \n",
      "322 | bert.bert.encoder.layer.18.attention.self.dropout     | Dropout                       | 0     \n",
      "323 | bert.bert.encoder.layer.18.attention.output           | BertSelfOutput                | 1 M   \n",
      "324 | bert.bert.encoder.layer.18.attention.output.dense     | Linear                        | 1 M   \n",
      "325 | bert.bert.encoder.layer.18.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "326 | bert.bert.encoder.layer.18.attention.output.dropout   | Dropout                       | 0     \n",
      "327 | bert.bert.encoder.layer.18.intermediate               | BertIntermediate              | 4 M   \n",
      "328 | bert.bert.encoder.layer.18.intermediate.dense         | Linear                        | 4 M   \n",
      "329 | bert.bert.encoder.layer.18.output                     | BertOutput                    | 4 M   \n",
      "330 | bert.bert.encoder.layer.18.output.dense               | Linear                        | 4 M   \n",
      "331 | bert.bert.encoder.layer.18.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "332 | bert.bert.encoder.layer.18.output.dropout             | Dropout                       | 0     \n",
      "333 | bert.bert.encoder.layer.19                            | BertLayer                     | 12 M  \n",
      "334 | bert.bert.encoder.layer.19.attention                  | BertAttention                 | 4 M   \n",
      "335 | bert.bert.encoder.layer.19.attention.self             | BertSelfAttention             | 3 M   \n",
      "336 | bert.bert.encoder.layer.19.attention.self.query       | Linear                        | 1 M   \n",
      "337 | bert.bert.encoder.layer.19.attention.self.key         | Linear                        | 1 M   \n",
      "338 | bert.bert.encoder.layer.19.attention.self.value       | Linear                        | 1 M   \n",
      "339 | bert.bert.encoder.layer.19.attention.self.dropout     | Dropout                       | 0     \n",
      "340 | bert.bert.encoder.layer.19.attention.output           | BertSelfOutput                | 1 M   \n",
      "341 | bert.bert.encoder.layer.19.attention.output.dense     | Linear                        | 1 M   \n",
      "342 | bert.bert.encoder.layer.19.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "343 | bert.bert.encoder.layer.19.attention.output.dropout   | Dropout                       | 0     \n",
      "344 | bert.bert.encoder.layer.19.intermediate               | BertIntermediate              | 4 M   \n",
      "345 | bert.bert.encoder.layer.19.intermediate.dense         | Linear                        | 4 M   \n",
      "346 | bert.bert.encoder.layer.19.output                     | BertOutput                    | 4 M   \n",
      "347 | bert.bert.encoder.layer.19.output.dense               | Linear                        | 4 M   \n",
      "348 | bert.bert.encoder.layer.19.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "349 | bert.bert.encoder.layer.19.output.dropout             | Dropout                       | 0     \n",
      "350 | bert.bert.encoder.layer.20                            | BertLayer                     | 12 M  \n",
      "351 | bert.bert.encoder.layer.20.attention                  | BertAttention                 | 4 M   \n",
      "352 | bert.bert.encoder.layer.20.attention.self             | BertSelfAttention             | 3 M   \n",
      "353 | bert.bert.encoder.layer.20.attention.self.query       | Linear                        | 1 M   \n",
      "354 | bert.bert.encoder.layer.20.attention.self.key         | Linear                        | 1 M   \n",
      "355 | bert.bert.encoder.layer.20.attention.self.value       | Linear                        | 1 M   \n",
      "356 | bert.bert.encoder.layer.20.attention.self.dropout     | Dropout                       | 0     \n",
      "357 | bert.bert.encoder.layer.20.attention.output           | BertSelfOutput                | 1 M   \n",
      "358 | bert.bert.encoder.layer.20.attention.output.dense     | Linear                        | 1 M   \n",
      "359 | bert.bert.encoder.layer.20.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "360 | bert.bert.encoder.layer.20.attention.output.dropout   | Dropout                       | 0     \n",
      "361 | bert.bert.encoder.layer.20.intermediate               | BertIntermediate              | 4 M   \n",
      "362 | bert.bert.encoder.layer.20.intermediate.dense         | Linear                        | 4 M   \n",
      "363 | bert.bert.encoder.layer.20.output                     | BertOutput                    | 4 M   \n",
      "364 | bert.bert.encoder.layer.20.output.dense               | Linear                        | 4 M   \n",
      "365 | bert.bert.encoder.layer.20.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "366 | bert.bert.encoder.layer.20.output.dropout             | Dropout                       | 0     \n",
      "367 | bert.bert.encoder.layer.21                            | BertLayer                     | 12 M  \n",
      "368 | bert.bert.encoder.layer.21.attention                  | BertAttention                 | 4 M   \n",
      "369 | bert.bert.encoder.layer.21.attention.self             | BertSelfAttention             | 3 M   \n",
      "370 | bert.bert.encoder.layer.21.attention.self.query       | Linear                        | 1 M   \n",
      "371 | bert.bert.encoder.layer.21.attention.self.key         | Linear                        | 1 M   \n",
      "372 | bert.bert.encoder.layer.21.attention.self.value       | Linear                        | 1 M   \n",
      "373 | bert.bert.encoder.layer.21.attention.self.dropout     | Dropout                       | 0     \n",
      "374 | bert.bert.encoder.layer.21.attention.output           | BertSelfOutput                | 1 M   \n",
      "375 | bert.bert.encoder.layer.21.attention.output.dense     | Linear                        | 1 M   \n",
      "376 | bert.bert.encoder.layer.21.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "377 | bert.bert.encoder.layer.21.attention.output.dropout   | Dropout                       | 0     \n",
      "378 | bert.bert.encoder.layer.21.intermediate               | BertIntermediate              | 4 M   \n",
      "379 | bert.bert.encoder.layer.21.intermediate.dense         | Linear                        | 4 M   \n",
      "380 | bert.bert.encoder.layer.21.output                     | BertOutput                    | 4 M   \n",
      "381 | bert.bert.encoder.layer.21.output.dense               | Linear                        | 4 M   \n",
      "382 | bert.bert.encoder.layer.21.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "383 | bert.bert.encoder.layer.21.output.dropout             | Dropout                       | 0     \n",
      "384 | bert.bert.encoder.layer.22                            | BertLayer                     | 12 M  \n",
      "385 | bert.bert.encoder.layer.22.attention                  | BertAttention                 | 4 M   \n",
      "386 | bert.bert.encoder.layer.22.attention.self             | BertSelfAttention             | 3 M   \n",
      "387 | bert.bert.encoder.layer.22.attention.self.query       | Linear                        | 1 M   \n",
      "388 | bert.bert.encoder.layer.22.attention.self.key         | Linear                        | 1 M   \n",
      "389 | bert.bert.encoder.layer.22.attention.self.value       | Linear                        | 1 M   \n",
      "390 | bert.bert.encoder.layer.22.attention.self.dropout     | Dropout                       | 0     \n",
      "391 | bert.bert.encoder.layer.22.attention.output           | BertSelfOutput                | 1 M   \n",
      "392 | bert.bert.encoder.layer.22.attention.output.dense     | Linear                        | 1 M   \n",
      "393 | bert.bert.encoder.layer.22.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "394 | bert.bert.encoder.layer.22.attention.output.dropout   | Dropout                       | 0     \n",
      "395 | bert.bert.encoder.layer.22.intermediate               | BertIntermediate              | 4 M   \n",
      "396 | bert.bert.encoder.layer.22.intermediate.dense         | Linear                        | 4 M   \n",
      "397 | bert.bert.encoder.layer.22.output                     | BertOutput                    | 4 M   \n",
      "398 | bert.bert.encoder.layer.22.output.dense               | Linear                        | 4 M   \n",
      "399 | bert.bert.encoder.layer.22.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "400 | bert.bert.encoder.layer.22.output.dropout             | Dropout                       | 0     \n",
      "401 | bert.bert.encoder.layer.23                            | BertLayer                     | 12 M  \n",
      "402 | bert.bert.encoder.layer.23.attention                  | BertAttention                 | 4 M   \n",
      "403 | bert.bert.encoder.layer.23.attention.self             | BertSelfAttention             | 3 M   \n",
      "404 | bert.bert.encoder.layer.23.attention.self.query       | Linear                        | 1 M   \n",
      "405 | bert.bert.encoder.layer.23.attention.self.key         | Linear                        | 1 M   \n",
      "406 | bert.bert.encoder.layer.23.attention.self.value       | Linear                        | 1 M   \n",
      "407 | bert.bert.encoder.layer.23.attention.self.dropout     | Dropout                       | 0     \n",
      "408 | bert.bert.encoder.layer.23.attention.output           | BertSelfOutput                | 1 M   \n",
      "409 | bert.bert.encoder.layer.23.attention.output.dense     | Linear                        | 1 M   \n",
      "410 | bert.bert.encoder.layer.23.attention.output.LayerNorm | LayerNorm                     | 2 K   \n",
      "411 | bert.bert.encoder.layer.23.attention.output.dropout   | Dropout                       | 0     \n",
      "412 | bert.bert.encoder.layer.23.intermediate               | BertIntermediate              | 4 M   \n",
      "413 | bert.bert.encoder.layer.23.intermediate.dense         | Linear                        | 4 M   \n",
      "414 | bert.bert.encoder.layer.23.output                     | BertOutput                    | 4 M   \n",
      "415 | bert.bert.encoder.layer.23.output.dense               | Linear                        | 4 M   \n",
      "416 | bert.bert.encoder.layer.23.output.LayerNorm           | LayerNorm                     | 2 K   \n",
      "417 | bert.bert.encoder.layer.23.output.dropout             | Dropout                       | 0     \n",
      "418 | bert.bert.pooler                                      | BertPooler                    | 1 M   \n",
      "419 | bert.bert.pooler.dense                                | Linear                        | 1 M   \n",
      "420 | bert.bert.pooler.activation                           | Tanh                          | 0     \n",
      "421 | bert.dropout                                          | Dropout                       | 0     \n",
      "422 | bert.classifier                                       | Linear                        | 1 K   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation dataset has 1101 examples\n",
      "INFO:finetune_bert_module:Validation dataset has 1101 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training dataset has 8544 examples\n",
      "INFO:finetune_bert_module:Training dataset has 8544 examples\n",
      "Validation dataset has 1101 examples\n",
      "INFO:finetune_bert_module:Validation dataset has 1101 examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.068\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a9b467563c41eab81af3f1b020f4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 5.93 GiB total capacity; 4.44 GiB already allocated; 122.75 MiB free; 4.47 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8e2d76e58883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no-cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_core_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0m_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;31m# check if loss or model weights are nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    558\u001b[0m                                                                     opt_idx, self.hiddens)\n\u001b[1;32m    559\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                             \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                         \u001b[0;31m# format and reduce outputs accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_forward\u001b[0;34m(self, batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer_batch_to_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;31m# TPU support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/School/data_mining/CS391D_Final_Project/code/real_code/structural-probes/finetuning/finetune_bert_module.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_correctness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/School/data_mining/CS391D_Final_Project/code/real_code/structural-probes/finetuning/finetune_bert_module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mbert_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#output = bert_out[0].squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1174\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m         )\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         )\n\u001b[1;32m    792\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-mining/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 5.93 GiB total capacity; 4.44 GiB already allocated; 122.75 MiB free; 4.47 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "if testing:\n",
    "    trainer.test(model)\n",
    "else:\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_path = f'../best_models/{args.run_name}'\n",
    "    os.mkdir(model_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model.bert.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "data-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
