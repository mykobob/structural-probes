{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/06129/mbli/structural-probes\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/06129/mbli/maverick2/anaconda3/envs/dm/lib/python3.8/site-packages/jupyter_slack/jupyter_slack.py:17: UserWarning: Either $SLACK_WEBHOOK_URL must be set (see https://api.slack.com/messaging/webhooks) or both $SLACK_TOKEN and $SLACK_ID must be set (see https://api.slack.com/custom-integrations/legacy-tokens). All notifications will be noops under the current setting.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('structural-probes')\n",
    "sys.path.append('finetuning')\n",
    "from pathlib import Path\n",
    "\n",
    "from run_experiment import setup_new_experiment_dir, execute_experiment\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import eval_probes_on_dataset\n",
    "import jupyter_slack\n",
    "from utils import setup_runs\n",
    "import finetune_bert_module\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing new results directory at results/BERT-disk-parse-distance-2020-5-14-21-54-58-596289/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6f78c1e5b83c>:16: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_args = yaml.load(open(cli_args.experiment_config))\n"
     ]
    }
   ],
   "source": [
    "def setup_args_and_folder(): \n",
    "    CONFIG_FILE = 'configs/bert_base_distance_ptb3.yaml'\n",
    "    EXPERIMENT_NAME = ''\n",
    "    SEED = 123\n",
    "\n",
    "    class Object(object):\n",
    "        pass\n",
    "\n",
    "    cli_args = Object()\n",
    "    cli_args.experiment_config = CONFIG_FILE\n",
    "    cli_args.results_dir = EXPERIMENT_NAME\n",
    "    cli_args.train_probe = -1\n",
    "    cli_args.report_results = 1\n",
    "    cli_args.seed = SEED\n",
    "\n",
    "    yaml_args = yaml.load(open(cli_args.experiment_config))\n",
    "    setup_new_experiment_dir(cli_args, yaml_args, cli_args.results_dir)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    yaml_args['device'] = device\n",
    "    yaml_args['model_type'] = 'large'\n",
    "    return yaml_args\n",
    "\n",
    "yaml_args = setup_args_and_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements before running this code\n",
    "- Corpus (conllx, for BERT)\n",
    "    - Run `convert_splits_to_depparse.sh` (to get .conllx format)\n",
    "    - Run `convert_conll_to_raw.py` (to get into raw text) \n",
    "- BERT-layer embeddings (.bert-layers.hdf5)\n",
    "    - Run `convert_raw_to_bert.py` (Uses BERT to create bert-embeddings for ALL layers)\n",
    "- Depth & Distance Params Path\n",
    "    - Pretrained from data\n",
    "- Ground Truths\n",
    "    - Trees\n",
    "        - Run Stanford CoreNLP's `ReadSentimentDataset` to get the ground truth trees\n",
    "    - Sentiment\n",
    "        - Run `apply_splits.py` to average all phrases' sentiments in the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for generating the Tree data for SST\n",
    "- This is for reading in the Ground Truth trees that is already given to us in SST\n",
    "- We'll use Stanford's CoreNLP tools\n",
    "- Run the ReadSentimentDataset `java -mx4g edu.stanford.nlp.sentiment.ReadSentimentDataset -inputDir data/SST-2/original -outputDir tmp/`\n",
    "  - The ground truth already does subword partitions, so need to account for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "\n",
    "from reporter import WordPairReporter, WordReporter, prims_matrix_to_edges\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trees(path):\n",
    "    with open(path) as f:\n",
    "        tree_lines = f.readlines()\n",
    "        \n",
    "    trees = [Tree.fromstring(treeline) for treeline in tree_lines]\n",
    "    return trees\n",
    "\n",
    "def read_sentiment_sentences(path):\n",
    "    with open(path) as f:\n",
    "        all_sentences = f.readlines()\n",
    "        \n",
    "        sentences, labels = [], []\n",
    "        for pair in all_sentences:\n",
    "            sentence, label = pair.split('\\t')\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "        return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = Path('data/SST-2')\n",
    "\n",
    "train_path = data_base / 'sentence_splits' / 'train_cat.tsv'\n",
    "dev_path = data_base / 'sentence_splits' / 'dev_cat.tsv'\n",
    "\n",
    "# read in SST dataset\n",
    "sst_trees_base = data_base / 'tree_format/'\n",
    "gt_tree_train_path = sst_trees_base / 'train.txt'\n",
    "gt_tree_dev_path = sst_trees_base / 'dev.txt'\n",
    "\n",
    "train_sentiment, train_labels = read_sentiment_sentences(train_path)\n",
    "dev_sentiment, dev_labels = read_sentiment_sentences(dev_path)\n",
    "\n",
    "# Read into NTLK Trees\n",
    "gt_train_trees = read_trees(gt_tree_train_path)\n",
    "gt_dev_trees = read_trees(gt_tree_dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home1/06129/mbli/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home1/06129/mbli/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home1/06129/mbli/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home1/06129/mbli/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home1/06129/mbli/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "from finetune_bert_module import SST_Test\n",
    "\n",
    "desired_params = {\n",
    "    'sst_train_path': os.path.join(\"data\", \"SST-2\", \"sentence_splits\", \"train_cat.tsv\"),\n",
    "    'sst_val_path': os.path.join(\"data\", \"SST-2\", \"sentence_splits\", \"dev_cat.tsv\"),\n",
    "}\n",
    "hparams, params = setup_runs.get_default_args({}, desired_params)\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-cased')\n",
    "config.output_hidden_states=True\n",
    "config.num_labels = 1\n",
    "model = finetune_bert_module.SST_Test.load_from_checkpoint(\n",
    "        'finetuning/lightning_logs/proper_classification/_ckpt_epoch_0.ckpt', None, None, params)\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-large-cased', config=config)\n",
    "model = model.to(yaml_args['device'])\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[demoing]: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing TwoWordPSDProbe\n",
      "Constructing OneWordPSDProbe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[demoing]: 1101it [00:15, 70.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate distance between the two trees\n",
    "# one tree's format is from structural probes\n",
    "# other tree's format is from the dataset (gt_*_trees)\n",
    "# import importlib\n",
    "# importlib.reload(eval_probes_on_dataset)\n",
    "\n",
    "word_dists, word_depths, predicted_edges = eval_probes_on_dataset.use_probes(yaml_args, dev_sentiment, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Dev Sentiment: 1101it [00:12, 86.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy of the model\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "preds = []\n",
    "labels = [] \n",
    "for idx, line in tqdm(enumerate(dev_sentiment), desc='Eval Dev Sentiment'):\n",
    "    _, tokens_tensor, segments_tensors = eval_probes_on_dataset.prepare_sentence_for_bert(line, tokenizer)\n",
    "    line_label = int(dev_labels[idx])\n",
    "\n",
    "    device = yaml_args['device']\n",
    "    tokens_tensor = tokens_tensor.unsqueeze(0).to(device)\n",
    "    segments_tensors = segments_tensors.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits, encoded_layers = model(tokens_tensor, segments_tensors)\n",
    "        probs = softmax(logits)\n",
    "        preds.append(0 if probs[0][0] >= probs[0][1] else 1)\n",
    "    labels.append(line_label)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.8251121076233184\n"
     ]
    }
   ],
   "source": [
    "print('f1', f1_score(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 1,101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "# temp_root_path = \"/home/garysnake/Desktop/structural-probes/experiments/data/cola_public\"\n",
    "temp_data_path = \"data/SST-2/sentence_splits/dev_cat.tsv\"\n",
    "df = pd.read_csv(temp_data_path, delimiter='\\t', header=None, names=['sentence', 'label'])\n",
    "b\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 1,101 test sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/06129/mbli/maverick2/anaconda3/envs/dm/lib/python3.8/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Tracking variables \n",
    "# predictions , true_labels = [], []\n",
    "batch_accuracy = []\n",
    "mcc = []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits, encoded_layers = model.forward(b_input_ids, attn_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # Record batch accuracy\n",
    "    batch_accuracy.append(flat_accuracy(logits, label_ids))\n",
    "\n",
    "    # Record batch mcc  \n",
    "    mcc.append(matthews_corrcoef(label_ids, np.argmax(logits, axis=1).flatten()))\n",
    "      \n",
    "    # Store predictions and true labels\n",
    "#     predictions.append(logits)\n",
    "#     true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Scatter plots of UUAS v Accuracy & Spearmanr vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'probe_scores/dev.spearmanr_batch_average'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-12fe05b73192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mbatch_spearman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'probe_scores/dev.spearmanr_batch_average'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'probe_scores/dev.spearmanr_batch_average'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "#  matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_scatter(x, y, x_name, y_name):\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "    # Plot the learning curve.\n",
    "    plt.scatter(x, y)\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"{:} & {:}\".format(x_name, y_name))\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#  Next step export this set, and run probe on this dev\n",
    "# Next step might be running \n",
    "print(len(batch_accuracy))\n",
    "\n",
    "batch_uuas = []\n",
    "with open('probe_scores/dev.uuas','r') as f:\n",
    "    for line in f.readlines():\n",
    "        try:\n",
    "            batch_uuas.append(float(line))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "\n",
    "batch_spearman = []\n",
    "with open('probe_scores/dev.spearmanr_batch_average','r') as f:\n",
    "    for line in f.readlines():\n",
    "        try:\n",
    "            batch_spearman.append(float(line))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "\n",
    "plot_scatter(batch_accuracy, batch_uuas, \"accuracy\", \"uuas\")\n",
    "\n",
    "plot_scatter(batch_accuracy, batch_spearman, \"accuracy\", \"spearman\")\n",
    "\n",
    "plot_scatter(mcc, batch_uuas, \"mcc\", \"uuas\")\n",
    "\n",
    "plot_scatter(mcc, batch_spearman, \"mcc\", \"spearman\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
