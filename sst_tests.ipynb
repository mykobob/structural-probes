{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('structural-probes')\n",
    "from pathlib import Path\n",
    "\n",
    "from run_experiment import setup_new_experiment_dir, execute_experiment\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import eval_probes_on_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"../../..\")\n",
    "cola = data_folder / 'data'/ 'SST-2' / 'original' / 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing new results directory at results/BERT-disk-parse-distance-2020-4-21-11-8-5-352057/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mli/anaconda3/envs/data-mining/lib/python3.6/site-packages/ipykernel_launcher.py:16: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def setup_args_and_folder(): \n",
    "    CONFIG_FILE = 'example/config/bert_ptb3.yaml'\n",
    "    EXPERIMENT_NAME = ''\n",
    "    SEED = 123\n",
    "\n",
    "    class Object(object):\n",
    "        pass\n",
    "\n",
    "    cli_args = Object()\n",
    "    cli_args.experiment_config = CONFIG_FILE\n",
    "    cli_args.results_dir = EXPERIMENT_NAME\n",
    "    cli_args.train_probe = -1\n",
    "    cli_args.report_results = 1\n",
    "    cli_args.seed = SEED\n",
    "\n",
    "    yaml_args = yaml.load(open(cli_args.experiment_config))\n",
    "    setup_new_experiment_dir(cli_args, yaml_args, cli_args.results_dir)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    yaml_args['device'] = device\n",
    "    return yaml_args\n",
    "\n",
    "yaml_args = setup_args_and_folder()\n",
    "\n",
    "sentences = [\"Yet the act is still charming here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "[demoing]: 1it [00:00, 2129.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing TwoWordPSDProbe\n",
      "Constructing OneWordPSDProbe\n",
      "tokens ['[CLS]', 'Yet', 'the', 'act', 'is', 'still', 'charming', 'here', '[SEP]']\n",
      "torch.Size([1, 9, 1024])\n",
      "defaultdict(<class 'list'>, {0: [1], 1: [2], 2: [3], 3: [4], 4: [5], 5: [6], 6: [7]})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_dists, word_depths, predicted_edges = eval_probes_on_dataset.report_on_stdin(yaml_args, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1053205 4.2791424 2.2283788 2.2952678 1.7218056 1.3695142 1.5609764]\n",
      "[(2, 5), (1, 2), (3, 5), (4, 5), (5, 6), (0, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(word_depths)\n",
    "\n",
    "print(predicted_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for generating the Tree data for SST\n",
    "- This is for reading in the Ground Truth trees that is already given to us in SST\n",
    "- We'll use Stanford's CoreNLP tools\n",
    "- Run the ReadSentimentDataset `java -mx4g -cp \"*\" edu.stanford.nlp.sentiment.ReadSentimentDataset -inputDir data/SST-2/original -outputDir tmp/`\n",
    "  - The ground truth already does subword partitions, so need to account for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trees(path):\n",
    "    with open(path) as f:\n",
    "        tree_lines = f.readlines()\n",
    "        \n",
    "    trees = [Tree.fromstring(treeline) for treeline in tree_lines]\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in SST dataset\n",
    "sst_trees_base = Path('../../../data/SST-2/tree_format/')\n",
    "\n",
    "gt_train_path = sst_trees_base / 'train.txt'\n",
    "gt_dev_path = sst_trees_base / 'dev.txt'\n",
    "\n",
    "# Read into NTLK Trees\n",
    "\n",
    "gt_train_trees = read_trees(train_path)\n",
    "gt_dev_trees = read_trees(dev_path)\n",
    "\n",
    "# Calculate distance between the two trees\n",
    "# one tree's format is from structural probes\n",
    "# other tree's format is from the dataset (gt_*_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "data-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
